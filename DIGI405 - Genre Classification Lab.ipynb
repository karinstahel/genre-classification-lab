{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51655f1c-23d9-45d0-9dd9-16afa391f778",
   "metadata": {},
   "source": [
    "# DIGI405 Lab - Genre classification of historical newspaper texts\n",
    "\n",
    "In this notebook we will train and test two different classifiers on a collection of texts from [historical New Zealand newspapers](https://paperspast.natlib.govt.nz/newspapers). \n",
    "\n",
    "Our aim is to build genre classification models that are independent of topic, so we will use features based on the structure and layout of the text (for example line widths), linguistic features (such as the frequency of certain parts-of-speech), and other text statistics.\n",
    "\n",
    "The data used in this notebook is originally sourced from the [National Library of New Zealand's Papers Past open data](https://natlib.govt.nz/about-us/open-data/papers-past-metadata/papers-past-newspaper-open-data-pilot/dataset-papers-past-newspaper-open-data-pilot). It consists of a small dataset of articles that have been pre-labelled with their genre and includes features related to line widths and offsets that have been extracted from the [METS/ALTO XML files](https://veridiansoftware.com/knowledge-base/metsalto/) for each newspaper.\n",
    "\n",
    "We will use [spaCy](https://spacy.io/) and [textstat](https://pypi.org/project/textstat/) to extract additional features and add them to our dataframe. We will then use [scikit-learn](https://scikit-learn.org/stable/) to train and test our models.\n",
    "\n",
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 0:</strong> Throughout the notebook there are defined tasks for you to do. Watch out for them - they will have a box around them like this! Make sure you take some notes as you go.\n",
    "</div>\n",
    "\n",
    "![National Library Papers Past](https://images.ctfassets.net/pwv49hug9jad/6tW2XbQ3rwBfOYilgpZmVQ/468368a1454e2201958401cab2ea7d79/guides-pp-open-data-feature-image.jpg?fm=webp)\n",
    "\n",
    "[Image source: natlib.govt.nz](https://natlib.govt.nz/about-us/open-data/papers-past-metadata/papers-past-newspaper-open-data-pilot/get-started-papers-past-newspaper-open-data-pilot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c691409-29ef-40dd-9a0c-b6352d6418fb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-disclosure",
   "metadata": {},
   "source": [
    "Make sure the libraries we will need in this notebook are installed (you only need to run this cell once):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textstat\n",
    "# !pip install seaborn\n",
    "# !pip install supertree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-volleyball",
   "metadata": {},
   "source": [
    "Now import the required libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d297294-06bf-41bf-a157-dd55226de21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "# Classifier training and evaluation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from supertree import SuperTree  \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# Feature extraction\n",
    "import spacy\n",
    "import textstat\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(\n",
    "    context=\"notebook\",\n",
    "    style=\"whitegrid\",\n",
    "    font=\"sans-serif\",\n",
    "    font_scale=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb30ec9-266f-4df9-acd6-8d0d2c78e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263e1848-5064-46dc-b8fb-a90e41ead2c5",
   "metadata": {},
   "source": [
    "## Load and explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb895d2-84af-4b02-9039-408685be95fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe\n",
    "df = pd.read_csv(\"/srv/source-data/paperspast_4genres.csv\", index_col = [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0188bb-112f-40c4-a27c-d4778964dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the count of articles by genre\n",
    "display(df.groupby([\"genre\"])[\"genre\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c554e5d4-b0e2-4165-9366-8394726b315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first ten rows of the dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864422c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the ID of the newspaper 'Northern Advocate' is 'NA', this has been read-in as NaN\n",
    "# View the problem by selecting rows where 'newspaper' column equals 'Northern Advocate'\n",
    "# Look at the 'newspaper_id' column\n",
    "\n",
    "mask = df[\"newspaper\"] == \"Northern Advocate\"\n",
    "display(df.loc[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can fix this by filling the newspaper_id column for our selected rows with the correct code 'NA'\n",
    "df.loc[mask, \"newspaper_id\"] = df.loc[mask, \"newspaper_id\"].fillna(\"NA\")\n",
    "display(df.loc[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of the articles in our dataset by newspaper\n",
    "\n",
    "plt.figure(figsize = (8, 4))\n",
    "sample_papers_unique = df[\"newspaper\"].nunique()\n",
    "print(\"-----------------------------------------------------\") \n",
    "print(f\"Number of newspaper titles in sample dataset: {sample_papers_unique}\") \n",
    "print(\"-----------------------------------------------------\") \n",
    "print(\"\") \n",
    "\n",
    "ax_1 = sns.countplot(x = \"newspaper\", \n",
    "                     data = df, \n",
    "                     order = df[\"newspaper\"].value_counts().index, \n",
    "                     color = \"#32a5fc\")\n",
    "ax_1.set_xlabel(\"Newspaper\", fontsize = 10)\n",
    "ax_1.set_ylabel(\"Count of articles\", fontsize = 10, labelpad = 9)\n",
    "ax_1.set_title(\"Distribution of articles by newspaper\", fontsize = 10)\n",
    "sns.despine(top = True, right = True, left = True, bottom = False, offset = None, trim = False)\n",
    "plt.xticks(rotation = 90, fontsize = 9)\n",
    "plt.yticks(fontsize = 9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's view the distribution of the articles in our dataset by year\n",
    "\n",
    "plt.figure(figsize = (8, 4))\n",
    "annual_df = (df.groupby(df[\"year\"])\n",
    "             [\"text\"].count().reset_index())\n",
    "ax_2 = sns.barplot(x = \"year\", \n",
    "                   y = \"text\", \n",
    "                   data = annual_df, \n",
    "                   color = \"#32a5fc\")\n",
    "ax_2.set_xlabel(\"Year\", fontsize = 10, labelpad = 9)\n",
    "ax_2.set_ylabel(\"Count of articles\", fontsize = 10, labelpad = 9)\n",
    "ax_2.set_title(\"Distribution of articles in dataset by year\", fontsize = 10)\n",
    "sns.despine(top = True, right = True, left = True, bottom = False, offset = None, trim = False)\n",
    "plt.xticks(rotation = 90, fontsize = 9)\n",
    "plt.yticks(fontsize = 9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30583aef-080f-427a-b883-6a7c7f633dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can display the full text of a selected article by dataframe index\n",
    "selected_index = 431\n",
    "\n",
    "print(f\"\\nGenre: {df['genre'].values[selected_index]}\\n\")\n",
    "print(\"==============\\n\")\n",
    "print(f\"Title:\\t\\t{df['title'].values[selected_index]}\")\n",
    "print(f\"Newspaper:\\t{df['newspaper'].values[selected_index]}\")\n",
    "print(f\"Date:\\t\\t{df['day'].values[selected_index]} / {df['month'].values[selected_index]} / { df['year'].values[selected_index]}\\n\")\n",
    "print(f\"{df['text'].values[selected_index]}\\n\")\n",
    "print(f\"You can view the scanned article on the Papers Past website. \"\n",
    "      f\"Follow the link below to see the scan of the original article.\\n{df['article_web'].values[selected_index]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe938d-098c-43a0-a6d2-525b66516574",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "You'll see from the above that there can be symbols and punctuation in the text that are the result of [OCR](https://en.wikipedia.org/wiki/Optical_character_recognition) errors. We will run a simple cleaner function over the text column of the dataframe to improve this and add the cleaned text to a new column. Before we remove punctuation, we will count the sentences and add this feature to the dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca10d6d-72cf-4383-b103-a1441004b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(df, column_name):\n",
    "    \"\"\"\n",
    "    Given a dataframe column of OCR text, count the sentences, question marks, quotation marks,\n",
    "    exclamation marks, and apostrophes and store these counts in new columns.\n",
    "    \n",
    "    Remove symbols using a regular expression and create a clean text column.\n",
    "\n",
    "    Return the updated dataframe.\n",
    "    \"\"\"\n",
    "    # A column of sentence count is added to the dataframe before punctuation is removed.\n",
    "    df[\"sentence_count\"] = df[column_name].apply(lambda x: textstat.sentence_count(x))\n",
    "\n",
    "    # Count occurrences of specific characters and add to new columns\n",
    "    df[\"freq_q_marks\"] = df[column_name].apply(lambda x: x.count(\"?\"))\n",
    "    df[\"freq_double_quotes\"] = df[column_name].apply(lambda x: x.count('\"'))\n",
    "    df[\"freq_exclam\"] = df[column_name].apply(lambda x: x.count(\"!\"))\n",
    "    df[\"freq_apost\"] = df[column_name].apply(lambda x: x.count(\"'\"))\n",
    "\n",
    "    # Regex pattern for only alphanumeric text\n",
    "    pattern = re.compile(r\"[A-Za-z0-9]{1,50}\")\n",
    "    df[\"clean_text\"] = df[column_name].str.findall(pattern).str.join(\" \")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a4177-4db9-4c31-aa96-f63252ea5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleaner(df, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33cac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at that same text after 'cleaning'\n",
    "\n",
    "# We can display the full text of a selected article by index\n",
    "print(f\"\\nGenre: {df['genre'].values[selected_index]}\\n\")\n",
    "print(\"==============\\n\")\n",
    "print(f\"Title:\\t\\t{df['title'].values[selected_index]}\")\n",
    "print(f\"Newspaper:\\t{df['newspaper'].values[selected_index]}\")\n",
    "print(f\"Date:\\t\\t{df['day'].values[selected_index]} / {df['month'].values[selected_index]} / {df['year'].values[selected_index]}\\n\")\n",
    "print(df[\"clean_text\"].values[selected_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-values",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 1:</strong> We have done a very simple clean-up of the text but, as you can see, there are still problems. You might see incorrect or missing letters, capital letters in the wrong place, numbers where there should be letters, and more. Think about the impact this might have on our model and discuss it with your classmates or tutors. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134bbfbb-465d-4c63-9d76-5ac2ed780d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see our additional columns have been added to the end of our dataframe\n",
    "# Scroll across to the right if they are not visible\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574a183-116d-46cf-ac2d-5aaab1b95b82",
   "metadata": {},
   "source": [
    "## Feature extraction: linguistic features and text statistics\n",
    "\n",
    "The following cells extract parts-of-speech and text statistic features and add them to the dataframe. For efficiency, the texts are [processed for parts-of-speech tagging](https://spacy.io/usage/processing-pipelines) as a stream using spaCy's [nlp.pipe](https://spacy.io/usage/processing-pipelines#processing). This allows the texts to be buffered in batches instead of one-by-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b058fe54-7617-47b2-9bb5-bbc9caf4f5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to define the list of POS tags to count (you don't need to change anything here)\n",
    "# We will use a selection of Universal POS tags: https://universaldependencies.org/u/pos/ \n",
    "\n",
    "pos_tags = [\n",
    "            \"ADJ\",    # adjective\n",
    "            \"ADV\",    # adverb\n",
    "            \"NOUN\",   # noun\n",
    "            \"NUM\",    # numeral\n",
    "            \"PRON\",   # pronoun\n",
    "            \"PROPN\",  # proper noun\n",
    "            \"VERB\",   # verb\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797ee69-8406-4d0b-a157-a85e7dbbc29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(df, pos_tags):\n",
    "    \"\"\"\n",
    "    Given a pandas dataframe with a column called \"clean_text\"\n",
    "    and a list of Universal parts of speech tags, add columns for \n",
    "    a list of tokens, word count, relative frequencies \n",
    "    of the given parts of speech (using Spacy), relative frequencies \n",
    "    of stopwords, and relative frequencies of monosyllabic words \n",
    "    and selected punctuation. \n",
    "\n",
    "    Return the dataframe with the additional columns.\n",
    "    \"\"\"\n",
    "    stop = stopwords.words(\"english\")\n",
    " \n",
    "    token_list = []\n",
    "    pos_counts = []\n",
    "    input_col = \"clean_text\"\n",
    "\n",
    "    # Spacy pipeline to count POS\n",
    "    nlp_text_pipe = nlp.pipe(df[input_col], batch_size = 20)\n",
    "\n",
    "    for doc in nlp_text_pipe:\n",
    "        token_list.append([token.text for token in doc if not token.is_punct and not token.is_space]) \n",
    "        pos_counts.append(Counter(token.pos_ for token in doc if token.pos_ in pos_tags))\n",
    "    \n",
    "    df[\"tokens\"] = token_list\n",
    "    df[\"word_count\"] = df[\"tokens\"].apply(lambda x: len(x))\n",
    "    df[\"stopwords_count\"] = df[input_col].apply(lambda x: len([i for i in x.split() if i.lower() in stop]))\n",
    "    df[\"stopword_relfreq\"] = df[\"stopwords_count\"] / df[\"word_count\"]\n",
    "\n",
    "    pos_columns = set().union(*pos_counts)\n",
    "\n",
    "    # Compute the relative frequencies of each part-of-speech tag\n",
    "    for pos in pos_columns:\n",
    "        df[pos + \"_relfreq\"] = [count.get(pos, 0) for count in pos_counts] / df[\"word_count\"]\n",
    "\n",
    "    # Add relative frequency of monsyllabic words using the textstat library\n",
    "    # Add relative frequencies of the punctuation marks counted earlier\n",
    "    df[\"monosyll_count\"] = df[input_col].apply(lambda x: textstat.monosyllabcount(x)) \n",
    "    df[\"monosyll_relfreq\"] = df[\"monosyll_count\"] / df[\"word_count\"]\n",
    "    df[\"q_marks_relfreq\"] = df[\"freq_q_marks\"]  / df[\"word_count\"]\n",
    "    df[\"double_quotes_relfreq\"] = df[\"freq_double_quotes\"] / df[\"word_count\"]\n",
    "    df[\"exclam_relfreq\"] = df[\"freq_exclam\"]  / df[\"word_count\"]\n",
    "    df[\"apost_relfreq\"] = df[\"freq_apost\"] / df[\"word_count\"]\n",
    "\n",
    "    # Drop count columns that are no longer required (they've been converted to relative frequencies)\n",
    "    df.drop(columns=[\"tokens\", \n",
    "                     \"monosyll_count\", \n",
    "                     \"stopwords_count\", \n",
    "                     \"freq_q_marks\", \n",
    "                     \"freq_double_quotes\", \n",
    "                     \"freq_exclam\", \n",
    "                     \"freq_apost\"], axis = 1, inplace = True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f62f4f9-4772-44f6-a9e6-b38dfbb71e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function to extract text features and add them to the dataframe\n",
    "# This might take a little while to run\n",
    "df = process_text(df, pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffa9212-9ade-4b1a-8952-711f629290a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first few rows of the dataframe to see the features that have been added\n",
    "# Scroll to the right\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6caab7b-7f86-436b-ba88-eca61579f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can examine use of a selected POS for a given dataframe index\n",
    "\n",
    "pos_var = 'PRON'\n",
    "my_ind = 4\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------#\n",
    "\n",
    "doc = nlp(df.iloc[my_ind][\"clean_text\"])\n",
    "my_text = df.iloc[my_ind][\"clean_text\"]\n",
    "stop = stopwords.words(\"english\")\n",
    "my_pos = [token.text for token in doc if token.pos_ == pos_var]\n",
    "my_stopwords = [text for text in my_text.split() if text.lower() in stop]\n",
    "\n",
    "print(f\"------------------\\nIndex: {my_ind}\\n------------------\")\n",
    "display(df.loc[[my_ind]])\n",
    "print(f\"\\n------------------\")\n",
    "print(f\"Word count: {df.iloc[my_ind]['word_count']}\")\n",
    "print(f\"\\n------------------\")\n",
    "print(f\"{pos_var} relative frequency: {df.iloc[my_ind][f'{pos_var}_relfreq']:.3f}\")\n",
    "print(f\"\\n------------------\")\n",
    "print(f\"Article title:\\t\\t\\t{df.iloc[my_ind]['title']}\")\n",
    "print(f\"Scanned newspaper issue:\\t{df.iloc[my_ind]['newspaper_web']}\\n\")\n",
    "print(f\"------------------\\nClean text:\\n\")\n",
    "print(df.iloc[my_ind][f\"clean_text\"])\n",
    "print(f\"\\n------------------\")\n",
    "print(f\"{pos_var} (count = {len(my_pos)}):\\n\")\n",
    "print(my_pos)\n",
    "print(\"\\n------------------\\n\"\n",
    "        f\"Stopwords (count = {len(my_stopwords)}):\\n\")\n",
    "print(my_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also inspect summary statistics for all our numerical data \n",
    "# We will use these later to explore the misclassified articles\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-pound",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 2:</strong> Why is exploratory data anlaysis (EDA) using techniques such as visualising the data and examining descriptive statistics important? What can it reveal? Discuss with your classmates or tutors. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec64148-818c-4c2f-98ee-8a5f2a0a0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the full list of columns in our dataframe, and see their data types\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3724c5f6-f6aa-4e4a-91d1-13169e5edd6f",
   "metadata": {},
   "source": [
    "## Specify features to include in the model\n",
    "\n",
    "* We now need to specify the features we want to include in our model, for example if we know that two features are highly correlated, we can choose to only include one in the model\n",
    "* The line width and line offset features come from page layout data that is created when the newspapers are digitised. The unit of measure is 1/10th of millimeter. The \"offset\" is the distance from a text block's left edge to the start of the line of text.\n",
    "* You can include or remove features from the model to explore the impact of different combinations of features on the performance of the classifier.\n",
    "* **Use the default list shown below first, then experiment to see what effect the changes have on the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1ffc25-d663-4cc0-bc37-3ff6e377e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to include in the model \n",
    "# Place cursor in the text and press Ctrl + / to comment or uncomment the line\n",
    "\n",
    "features = [\n",
    "            \"avg_line_width\",\n",
    "            # \"min_line_width\",\n",
    "            # \"max_line_width\",\n",
    "            # \"line_width_range\",\n",
    "            \"avg_line_offset\",\n",
    "            # \"max_line_offset\",\n",
    "            # \"min_line_offset\",\n",
    "            # \"sentence_count\",\n",
    "            # \"word_count\",\n",
    "            # \"stopword_relfreq\",\n",
    "            \"VERB_relfreq\",\n",
    "            # \"ADV_relfreq\",\n",
    "            \"PRON_relfreq\",\n",
    "            \"ADJ_relfreq\",\n",
    "            # \"PROPN_relfreq\",\n",
    "            # \"NOUN_relfreq\",\n",
    "            \"NUM_relfreq\",\n",
    "            \"monosyll_relfreq\",\n",
    "            # \"q_marks_relfreq\",\n",
    "            # \"double_quotes_relfreq\",\n",
    "            # \"exclam_relfreq\",\n",
    "            # \"apost_relfreq\",\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd46cd83-e31d-4f1c-8673-890ee3649a9b",
   "metadata": {},
   "source": [
    "## Set the target genre\n",
    "\n",
    "* We will specify the genre we want to predict with the binary classifier. \n",
    "* The selected genre will be labelled as 1 in the binary classification model, with the other classes labelled as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a85462-bb01-4cee-8246-d40db029a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select from:\n",
    "# FamilyNotice     \n",
    "# Fiction          \n",
    "# LetterToEditor    \n",
    "# Poetry         \n",
    "\n",
    "target_genre = \"Poetry\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-peninsula",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 3:</strong> Train and test classifiers for each of the four genres and take note of the results in a separate document. Which combination of genre and classifier achieved the best metrics and which was the worst? Discuss the results with your classmates or tutors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898c3a6-3533-4464-bdfa-991f38b90ee5",
   "metadata": {},
   "source": [
    "## Split the data into train and test sets\n",
    "\n",
    "* Run the cells below to split the data into train and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc34dd20-47d5-4d2e-890d-41b5d299c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_data(df, features, target_genre):\n",
    "    \"\"\"\n",
    "    Given the dataframe, features to include in the model,\n",
    "    and the target genre, split the data into \n",
    "    training and test sets and use the dataframe indices to \n",
    "    save the order of the split\n",
    "    \"\"\"\n",
    "    \n",
    "    df[\"binary_class\"] = np.where(df[\"genre\"] == target_genre, 1, 0)\n",
    "    model_df = df.filter(features + [\"binary_class\"], axis=1)\n",
    "    indices = df.index.values\n",
    "\n",
    "    # Extract the explanatory variables in X and the target variable in y\n",
    "    y = model_df.binary_class.copy()\n",
    "    X = model_df.drop([\"binary_class\"], axis=1)\n",
    "\n",
    "    # Train test split \n",
    "    # Use the indices to save the order of the split.\n",
    "    # https://stackoverflow.com/questions/48947194/add-randomforestclassifier-predict-proba-results-to-original-dataframe\n",
    "    X_train, X_test, indices_train, indices_test = train_test_split(X, \n",
    "                                                                    indices, \n",
    "                                                                    test_size = .3,    # This value changes the proportion of data held out for the test set\n",
    "                                                                    random_state = 1)  # You can change the random state to change the allocation of docs to the training and test sets\n",
    "    \n",
    "    y_train, y_test = y[indices_train], y[indices_test]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, indices_train, indices_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030e1958-eaf9-43d1-8c7c-a8c5c3ea5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_data(df, features, target_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c5d5c-a77d-41a2-ad85-50a3bb574951",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139306fc-894f-453d-9b04-ab5417bd26a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4195a2c3-d6b1-4e3c-b612-012c519d3c25",
   "metadata": {},
   "source": [
    "## Train and Test a Logistic Regression Classifier \n",
    "* [Logistic regression](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression) is a classification method popular for its computational efficiency and interpretability.\n",
    "* Run the cells below to train and test a logistic regression classifier for our selected genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e518c7-006e-4c8f-abac-a7f978c091f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_binary(X_train, X_test, y_train, y_test, target_genre):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model to classify the selected genre\n",
    "    \"\"\" \n",
    "    pipe = Pipeline([(\"scl\", StandardScaler()),\n",
    "                     (\"clf\", LogisticRegression())]) \n",
    "    pipe.fit(X_train, y_train)  \n",
    "    \n",
    "    y_pred_train = pipe.predict(X_train)\n",
    "    y_pred_test = pipe.predict(X_test)\n",
    "    y_prob_train = pipe.predict_proba(X_train)\n",
    "    y_prob_test = pipe.predict_proba(X_test)\n",
    "        \n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(f\"Binary Classification - Logistic Regression\")\n",
    "    print(f\"Target genre: {target_genre}\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print()\n",
    "\n",
    "    # Mapping from numeric classes to descriptive names\n",
    "    class_mapping = {\n",
    "        0: \"Not \" + target_genre,\n",
    "        1: target_genre\n",
    "    }\n",
    "    \n",
    "    # Use this mapping to create target_names\n",
    "    target_classes = [0, 1]\n",
    "    target_names = [class_mapping[0], class_mapping[1]]\n",
    "\n",
    "    print(classification_report(y_test, y_pred_test, target_names=target_names))\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_pred_test, \n",
    "    display_labels=target_names, \n",
    "    cmap=\"Blues\")\n",
    "    plt.grid(False)  \n",
    "    plt.show()   \n",
    "    \n",
    "    print()\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(f\"Model coefficients \\nwith log odds (logit) converted to odds ratio\\nfor improved interpretability\\n\")\n",
    "    print(f\"Target genre: {target_genre}\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    # Get coefficients (log odds or logit)\n",
    "    log_odds = pipe.named_steps[\"clf\"].coef_[0]\n",
    "    # Convert log odds to odds ratio\n",
    "    odds = np.exp(log_odds)\n",
    "    \n",
    "    return y_pred_train, y_pred_test, y_prob_train, y_prob_test, log_odds, odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68553bc-821a-45be-a43d-8f16bb863b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genres_binary_lr(df, target_genre, X_train, X_test, y_train, y_test, indices_train, indices_test):\n",
    "    \"\"\"\n",
    "    Train and test the model, and return the dataframe\n",
    "    with appended predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred_train, y_pred_test, y_prob_train, y_prob_test, log_odds, odds = log_reg_binary(X_train, \n",
    "                                                                                          X_test,\n",
    "                                                                                          y_train,\n",
    "                                                                                          y_test,\n",
    "                                                                                          target_genre\n",
    "                                                                                         )\n",
    "\n",
    "    # # Add the predictions to a copy of the original dataframe\n",
    "    df_new = df.copy()\n",
    "    df_new.loc[indices_train,\"pred_train\"] = y_pred_train\n",
    "    df_new.loc[indices_test,\"pred_test\"] = y_pred_test\n",
    "    df_new.loc[indices_train,\"prob_0_train\"] = y_prob_train[:,0]\n",
    "    df_new.loc[indices_test,\"prob_0_test\"] = y_prob_test[:,0]\n",
    "    df_new.loc[indices_train,\"prob_1_train\"] = y_prob_train[:,1]\n",
    "    df_new.loc[indices_test,\"prob_1_test\"] = y_prob_test[:,1]   \n",
    "\n",
    "    # Sort the dataframe by probability of being the given genre\n",
    "    df_new = df_new.sort_values(by=\"prob_1_test\", ascending=False)  \n",
    "    \n",
    "    # Create a dataframe with both log odds and odds\n",
    "    lr_odds_df = pd.DataFrame({\n",
    "        \"feature\": X_train.columns,\n",
    "        \"log odds (logit)\": log_odds,\n",
    "        \"odds ratio\": odds\n",
    "    })\n",
    "    \n",
    "    # Sort the dataframe by odds in descending order\n",
    "    lr_odds_df = lr_odds_df.sort_values(by=\"odds ratio\", ascending=False)\n",
    "    \n",
    "    # Reset the index for cleaner display\n",
    "    lr_odds_df = lr_odds_df.reset_index(drop=True)\n",
    "       \n",
    "    return df_new, lr_odds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9fe472-694a-4fa9-861c-877eb7cb9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_preds_df, lr_odds_df = genres_binary_lr(df, \n",
    "                                           target_genre, \n",
    "                                           X_train, \n",
    "                                           X_test, \n",
    "                                           y_train, \n",
    "                                           y_test, \n",
    "                                           indices_train, \n",
    "                                           indices_test)\n",
    "\n",
    "# Explore the model coefficients\n",
    "display(lr_odds_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5b618-e940-44ef-8a9e-521fda5a50a6",
   "metadata": {},
   "source": [
    "### Interpreting the logistic regression model\n",
    "A benefit of logistic regression is that it is relatively easy to interpret compared to other classifiers. We can extract the coefficients of the features in the final model (using the 'coef_' attribute) to see which features were the strongest predictors of the positive class (in our case, the selected genre). \n",
    "\n",
    "The coefficients extracted using 'coef_' are the log odds (logit) that an observation belongs to the positive class. In order to interpret them more easily, we can convert them to the odds ratio. An odds ratio greater than 1 represents a positive association and can be interpreted as follows:\n",
    "\n",
    "**\"For every unit increase in {feature}, the odds of the article being classified as {positive class} increase by a factor of {odds ratio} holding all other variables constant.\"**\n",
    "\n",
    "An odds ratio less than 1 represents a negative association. To describe them in a similar way to the above, we need to take 1/odds ratio. For example:\n",
    "\n",
    "\"For every unit increase in {feature}, the odds that the observation **is not** classified as {positive class} increase by a factor of {1 / odds ratio}, holding all other variables constant.\"\n",
    "\n",
    "**When interpreting the model coefficients it is important to consider the influence of features that may be correlated with each other (multicollinearity). These features will have similar predictive relationships to the outcome and therefore the sign and value of the coefficients should be interpreted with caution.** \n",
    "\n",
    "You can read more about calculating and interpreting the coefficients of regression models in this [Towards Data Science](https://towardsdatascience.com/interpreting-coefficients-in-linear-and-logistic-regression-6ddf1295f6f1) article. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e66ee-a857-47e7-b128-2a4fa6555502",
   "metadata": {},
   "source": [
    "## Train and Test a Decision Tree classifier \n",
    "* [Decision Tree](https://scikit-learn.org/stable/modules/tree.html) methods are useful because they are very easy to apply and interpret, however, the results can be susceptible to small changes in the dataset and they don't work so well for imbalanced datasets (is this a problem with our dataset?).\n",
    "* Run the cells below to train and test a Decision Tree classifier for our selected genre and compare the results to the logistic regression model.\n",
    "* We are using the Gini index to measure the quality of the split. If you want to know more about this measure, you can see a worked calculation with Python [here](https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b044212-1a92-4a46-b872-d476424594fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_binary(X_train, X_test, y_train, y_test, target_genre, features):\n",
    "    \"\"\"\n",
    "    Train a decision tree to classify the selected genre\n",
    "    \"\"\" \n",
    "    pipe = Pipeline([(\"clf\", \n",
    "                      DecisionTreeClassifier(\n",
    "                          criterion = \"gini\", # Gini impurity - method for measuring the quality of the split\n",
    "                          random_state = 343, \n",
    "                          max_depth = 4 # Limiting the depth of the tree can help to prevent overfitting\n",
    "                      )\n",
    "                     )]) \n",
    "    pipe.fit(X_train, y_train)\n",
    "    tree_model = pipe.named_steps['clf']\n",
    "    y_pred_train = pipe.predict(X_train)\n",
    "    y_pred_test = pipe.predict(X_test)\n",
    "    \n",
    "    y_prob_train = pipe.predict_proba(X_train) \n",
    "    y_prob_test = pipe.predict_proba(X_test) \n",
    "\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(f\"Binary Classification - Decision Tree\")\n",
    "    print(f\"{target_genre}\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print()\n",
    "\n",
    "    # Ensure X_train and y_train have consistent indexing\n",
    "    X_train_reset = X_train.reset_index(drop=True)\n",
    "    y_train_reset = pd.Series(y_train).reset_index(drop=True)\n",
    "    \n",
    "    # Get feature names from df columns\n",
    "    feature_names = X_train.columns.tolist()\n",
    "    \n",
    "    # Mapping from numeric classes to descriptive names\n",
    "    class_mapping = {\n",
    "        0: \"Not \" + target_genre,\n",
    "        1: target_genre\n",
    "    }\n",
    "    \n",
    "    # Use this mapping to create target_names\n",
    "    target_names = [class_mapping[0], class_mapping[1]]\n",
    "\n",
    "    print(classification_report(y_test, y_pred_test, target_names=target_names))\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, y_pred_test, \n",
    "    display_labels=target_names, \n",
    "    cmap=\"Blues\")\n",
    "    plt.grid(False)  \n",
    "    plt.show()   \n",
    "    \n",
    "    st = SuperTree(\n",
    "        tree_model,\n",
    "        X_train_reset,  # Training data\n",
    "        y_train_reset,  # Target values\n",
    "        feature_names,  # Feature names from df\n",
    "        target_names,    # Class names \n",
    "    )\n",
    "        \n",
    "    return y_pred_train, y_pred_test, y_prob_train, y_prob_test, st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265ec28-6ad9-4bb4-9b0c-284f494f2618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genres_binary_dt(df, target_genre, X_train, X_test, y_train, y_test, indices_train, indices_test, features):\n",
    "    \"\"\"\n",
    "    Train and test the model, and return the dataframe\n",
    "    with appended predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    y_pred_train, y_pred_test, y_prob_train, y_prob_test, st = dt_binary(X_train, \n",
    "                                                                     X_test, \n",
    "                                                                     y_train, \n",
    "                                                                     y_test, \n",
    "                                                                     target_genre,\n",
    "                                                                     features)\n",
    "\n",
    "    # Add the predictions to a copy of the original dataframe\n",
    "    df_new = df.copy()\n",
    "    df_new.loc[indices_train,\"pred_train\"] = y_pred_train\n",
    "    df_new.loc[indices_test,\"pred_test\"] = y_pred_test\n",
    "    df_new.loc[indices_train,\"prob_0_train\"] = y_prob_train[:,0]\n",
    "    df_new.loc[indices_test,\"prob_0_test\"] = y_prob_test[:,0]\n",
    "    df_new.loc[indices_train,\"prob_1_train\"] = y_prob_train[:,1]\n",
    "    df_new.loc[indices_test,\"prob_1_test\"] = y_prob_test[:,1]    \n",
    "\n",
    "    # Sort the dataframe by probability of being the given genre\n",
    "    df_new = df_new.sort_values(by=\"prob_1_test\", ascending=False)  \n",
    "    \n",
    "    return df_new, st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96195ad4-befc-4622-a836-131060d8416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_preds_df, st = genres_binary_dt(df, \n",
    "                                   target_genre, \n",
    "                                   X_train, \n",
    "                                   X_test, \n",
    "                                   y_train, \n",
    "                                   y_test, \n",
    "                                   indices_train, \n",
    "                                   indices_test, \n",
    "                                   features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef7caf-ed78-4404-ae9c-e7402a8fcdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Supertree library to display an interactive tree\n",
    "# https://mljar.com/supertree/\n",
    "st.show_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c3161c-ff18-4318-a6f2-9a0f5587d86d",
   "metadata": {},
   "source": [
    "## Inspect incorrectly classified texts\n",
    "\n",
    "We can explore which texts were incorrectly classified by the two models. Run the cell below to display dataframes of the misclassified texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644d8eb-0dcb-4d59-81cd-690205cfd1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "lr_misclass = lr_preds_df.loc[(lr_preds_df[\"binary_class\"] != lr_preds_df[\"pred_test\"]) & (lr_preds_df[\"pred_test\"] >= 0)]\n",
    "lr_misclass = lr_misclass.filter([\"date\", \n",
    "                                  \"newspaper\", \n",
    "                                  \"title\", \n",
    "                                  \"clean_text\",\n",
    "                                  \"genre\", \n",
    "                                  \"binary_class\", \n",
    "                                  \"pred_test\", \n",
    "                                  \"article_web\"], \n",
    "                                  axis=1\n",
    "                                ).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nMisclassified texts for Logistic Regression model (lr)\")\n",
    "print(f\"{target_genre}\")\n",
    "print(\"========================================================\\n\")\n",
    "display(lr_misclass)\n",
    "\n",
    "dt_misclass = dt_preds_df.loc[(dt_preds_df[\"binary_class\"] != dt_preds_df[\"pred_test\"]) & (dt_preds_df[\"pred_test\"] >= 0)]\n",
    "dt_misclass = dt_misclass.filter([\"date\", \n",
    "                                  \"newspaper\", \n",
    "                                  \"title\", \n",
    "                                  \"clean_text\",\n",
    "                                  \"genre\", \n",
    "                                  \"binary_class\", \n",
    "                                  \"pred_test\", \n",
    "                                  \"article_web\"], \n",
    "                                  axis=1\n",
    "                                ).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nMisclassified texts for Decision Tree model (dt)\")\n",
    "print(f\"{target_genre}\")\n",
    "print(\"========================================================\\n\")\n",
    "display(dt_misclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599ce01-c104-494b-8532-75f222572c7a",
   "metadata": {},
   "source": [
    "### Display the full text, the feature values, and the newspaper web address of a selected misclassification by model and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340cb5b8-6a6c-4d80-875b-b8482c49d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and index number of the misclassified text\n",
    "\n",
    "# Enter 'lr' or 'dt'\n",
    "model_code = \"lr\"\n",
    "selected_index = 0\n",
    "\n",
    "##################################################################\n",
    "\n",
    "if model_code == \"lr\":\n",
    "    article_title = f\"{lr_misclass['title'].values[selected_index]}\"\n",
    "    article_text = f\"{lr_misclass['clean_text'].values[selected_index]}\"\n",
    "    print(\"\\nTitle:\")\n",
    "    print(\"--------------\")\n",
    "    print(article_title)\n",
    "    print(\"\\nCleaned text:\")\n",
    "    print(\"--------------\")\n",
    "    print(article_text)\n",
    "    print(\"\\nView the scanned article on the Papers Past website\")\n",
    "    print(f\"{lr_misclass['article_web'].values[selected_index]}\\n\") \n",
    "\n",
    "elif model_code == \"dt\":\n",
    "    article_title = f\"{dt_misclass['title'].values[selected_index]}\"\n",
    "    article_text = f\"{dt_misclass['clean_text'].values[selected_index]}\"\n",
    "    print(\"\\nTitle:\")\n",
    "    print(\"--------------\")\n",
    "    print(article_title)\n",
    "    print(\"\\nCleaned text:\")\n",
    "    print(\"--------------\")\n",
    "    print(article_text)\n",
    "    print(\"\\nView the scanned article on the Papers Past website\")\n",
    "    print(f\"{dt_misclass['article_web'].values[selected_index]}\\n\") \n",
    "else:\n",
    "    print(\"\\nPlease enter either 'lr' or 'dt' for the model code\")\n",
    "    \n",
    "mask = (df['title'] == article_title) & (df['clean_text'] == article_text)\n",
    "display(df.loc[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-strand",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid black;margin-top:1em;padding:0.5em;\">\n",
    "    <strong>Task 4:</strong> Examine some of the misclassified texts. Why do you think they were misclassified? Examine the coefficients of the logistic regression model or the decision tree nodes, and the feature values. You can compare the feature values for the important logistic regression features in selected article to the overall dataset distribution for that feature shown in the summary statistics (see cell below). Discuss with your classmates or tutors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect summary statistics for the whole dataset to compare to the misclassified text\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e7e53d-c2e6-4c40-a512-a028c998f7a6",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "In this section, we'll use multinomial logistic regression to classify the newspaper articles into our four genre categories simultaneously. Unlike binary classification which answers, \"Is this article genre X or not?\", the multiclass approach directly predicts which of several genres an article belongs to by calculating probabilities for each possible class and assigning the article to the genre with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca75741-e239-439f-bd28-22a252bfb76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_data_multiclass(df, features):\n",
    "    \"\"\"\n",
    "    Given the dataframe and features to include in the model,\n",
    "    split the data into training and test sets for multiclass classification\n",
    "    and use the dataframe indices to save the order of the split\n",
    "    \"\"\"  \n",
    "    all_features = features + [\"genre\"]\n",
    "    model_df = df[all_features]\n",
    "    indices = df.index.values\n",
    "\n",
    "    y = model_df.genre.copy()\n",
    "    X = model_df.drop([\"genre\"], axis=1)\n",
    "\n",
    "    # Train test split with stratification to maintain class distribution\n",
    "    X_train, X_test, indices_train, indices_test = train_test_split(\n",
    "        X, \n",
    "        indices, \n",
    "        test_size = 0.3,\n",
    "        random_state = 1,\n",
    "        stratify = y  # Stratify ensures balanced class distribution\n",
    "    )\n",
    "    \n",
    "    y_train, y_test = y[indices_train], y[indices_test]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, indices_train, indices_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa700bff-b4da-4b40-a5cc-72051bcc38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mc, X_test_mc, y_train_mc, y_test_mc, indices_train_mc, indices_test_mc = train_test_data_multiclass(df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac65ea08-cee2-4a95-826d-4b3545f04697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg_multiclass(X_train, X_test, y_train, y_test, indices_train, indices_test):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model for multiclass genre classification\n",
    "    \"\"\" \n",
    "    pipe = Pipeline([(\"scl\", StandardScaler()),\n",
    "                     (\"clf\", LogisticRegression())]) \n",
    "    pipe.fit(X_train, y_train)   \n",
    "    y_pred_train = pipe.predict(X_train)\n",
    "    y_pred_test = pipe.predict(X_test)\n",
    "    y_prob_train = pipe.predict_proba(X_train)\n",
    "    y_prob_test = pipe.predict_proba(X_test)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "    \n",
    "    classes = pipe.classes_\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred_test)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title('\\nConfusion Matrix - Multiclass Logistic Regression\\n')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred_train, y_pred_test, y_prob_train, y_prob_test, pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48bb335-52f7-4580-8bd6-9d985a333bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train_mc, y_pred_test_mc, y_prob_train_mc, y_prob_test_mc, model_mc = log_reg_multiclass(\n",
    "    X_train_mc, X_test_mc, y_train_mc, y_test_mc, indices_train_mc, indices_test_mc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badfb310-ac9f-4653-8c39-6ef761569523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (ipykernel)",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
